{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Kурс Open Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Соревнование \"Прогноз популярности статьи на Хабре\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Волков Дмитрий</b>\n",
    "\n",
    "Ссылка на конкурс: https://inclass.kaggle.com/c/howpop-habrahabr-favs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Загрузка данных в json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Сначала важный технический момент.</b>\n",
    "\n",
    "Я создавал 2 пары json файлов: train и test включающие содержание статей и не включающие (поле content). Работать с содержанием имеет смысл, только если у вас есть 16 Гб оперативной памяти. Иначе это будет очень долго и не факт что успешно (особенно под Windows). \n",
    "\n",
    "По умолчанию всё настроено на использование содержания. Если хотите изменить это - измените значение переменной ниже.\n",
    "\n",
    "Модель без использования содержания статей теряет примерно 0.05 и дает результат на прайват в районе 0.398"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Сделайте свой выбор сразу :)\n",
    "use_content = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ниже приведены пути к папкам с исходными данными и файлам которые будут сгенерированы: создайте у себя такую же структуру или поправьте пути. Также создайте папку /results для записи сабмишена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Пути к файлам\n",
    "# корневая папка с исходными данными\n",
    "data_root = \"./data/new\"\n",
    "# папка со всеми исходными json файлами для трейна\n",
    "train_dir = data_root + \"/train\"\n",
    "# папка со всеми исходными json файлами для теста\n",
    "test_dir = data_root + \"/test\"\n",
    "\n",
    "# файлы которые будут созданы при загрузке данных\n",
    "train_file_content = data_root + \"/train_new.json\"\n",
    "test_file_content = data_root + \"/test_new.json\"\n",
    "target_csv = data_root + \"/train_target.csv\"\n",
    "\n",
    "# имена json без содержания статей\n",
    "train_file_light = data_root + \"/train_new_light.json\"\n",
    "test_file_light = data_root + \"/test_new_light.json\"\n",
    "\n",
    "# сабмишн\n",
    "submission_file = './results/sub_ridge.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "numbers = re.compile(r'(\\d+)')\n",
    "\n",
    "def preprocess_json_row(json_df):\n",
    "    # формируем из 'hubs' [id] и [title] -> списки\n",
    "    row = json_df.loc[0, 'hubs']\n",
    "    hub_id_list, hub_title_list, hub_url_list = [], [], []\n",
    "    #строка - это список словарей. Получаем нужные нам поля\n",
    "    for dct in row:\n",
    "        hub_id_list.append(dct['id'])\n",
    "        hub_title_list.append(dct['title'])\n",
    "        hub_url_list.append(dct['url'])\n",
    "\n",
    "    json_df.loc[0, 'hub_id'] = ', '.join(hub_id_list)\n",
    "    json_df.loc[0, 'hub_title'] = ', '.join(hub_title_list)\n",
    "\n",
    "    return json_df\n",
    "\n",
    "def numericalSort(value):\n",
    "    parts = numbers.split(value)\n",
    "    parts[1::2] = map(int, parts[1::2])\n",
    "    return parts\n",
    "\n",
    "def create_json(is_train=True, use_content=False, firstN = 0):\n",
    "    current_dir = os.getcwd()\n",
    "    if is_train:\n",
    "        os.chdir(train_dir)\n",
    "    else:\n",
    "        os.chdir(test_dir)\n",
    "\n",
    "    train_df = pd.DataFrame()\n",
    "    train_df_all = pd.DataFrame()\n",
    "    for (i, file) in enumerate(sorted(glob.glob(\"*.json\"), key=numericalSort)):\n",
    "        json_df = pd.read_json(file, lines=True)\n",
    "        json_df = preprocess_json_row(json_df)\n",
    "        train_df = train_df.append(json_df, ignore_index=True)\n",
    "\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(file)\n",
    "            print('{} {} files processed'.format(datetime.datetime.now().time(), i + 1))\n",
    "\n",
    "        # производительность df.append сильно падает после нескольких тыс вызовов. \n",
    "        # будем пересоздавать train_df каждые 5000 строк, а обработанное складывать в train_df_all\n",
    "        if (i + 1) % 5000 == 0:\n",
    "            # заполним content_len и почистим content если надо\n",
    "            train_df['content_len'] = train_df['content'].astype(str).apply(len)\n",
    "            if not use_content:\n",
    "                train_df['content'] = ''\n",
    "            #сохраним в all\n",
    "            train_df_all = train_df_all.append(train_df, ignore_index=True)\n",
    "            # пересоздадим\n",
    "            del train_df\n",
    "            train_df = pd.DataFrame()\n",
    "\n",
    "    # заполним content_len и почистим content если надо\n",
    "    train_df['content_len'] = train_df['content'].astype(str).apply(len)\n",
    "    if not use_content:\n",
    "        train_df['content'] = ''\n",
    "    # сохраним в all\n",
    "    train_df_all = train_df_all.append(train_df, ignore_index=True)\n",
    "    \n",
    "    # дополнительно преобразуем часть столбцов\n",
    "    train_df_all['published'] = train_df_all['published'].apply(lambda x: x['$date'])\n",
    "    train_df_all['author_nickname'] = train_df_all['author'].apply(lambda x: x['nickname']).fillna('-')\n",
    "    train_df_all['author_name'] = train_df_all['author'].apply(lambda x: x['name']).fillna('-')\n",
    "    train_df_all['author_url'] = train_df_all['author'].apply(lambda x: x['url']).fillna('-')\n",
    "\n",
    "    # добавляем 'flags', 'tags' -> список\n",
    "    train_df_all['flags'] = train_df_all['flags'].apply(lambda x: ', '.join(x))\n",
    "    train_df_all['tags'] = train_df_all['tags'].apply(lambda x: ', '.join(x))\n",
    "\n",
    "    train_df_all = train_df_all.drop(['hubs'], axis=1)\n",
    "    os.chdir(current_dir)\n",
    "\n",
    "    # пишем\n",
    "    train_file = train_file_content if use_content else train_file_light\n",
    "    test_file = test_file_content if use_content else test_file_light\n",
    "    \n",
    "    if is_train:\n",
    "        target = pd.read_csv(target_csv, index_col='_id')\n",
    "        train_df_all['favs_lognorm'] = train_df_all['_id'].map(target['favs_lognorm'])\n",
    "        train_df_all.to_json(train_file)\n",
    "        print('{} created'.format(train_file))\n",
    "    else:\n",
    "        train_df_all.to_json(test_file)\n",
    "        print('{} created'.format(test_file))        \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Создадим 2 json файла: для train и для test. Это может занять минут 40-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if use_content:\n",
    "    create_json(is_train=False, use_content=True)\n",
    "    create_json(is_train=True, use_content=True)\n",
    "else:\n",
    "    # для генерации json без содержания статей\n",
    "    create_json(is_train=False, use_content=False)\n",
    "    create_json(is_train=True, use_content=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "\n",
    "import re\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    # используем ли содержание статей (сильно всё замедляет, но добавляет ~0.05 к результату)\n",
    "    # чтобы запустить модель без использования содержания статей, сначала нужно дополнительно сгенерить соответствующую пару json\n",
    "    'LOAD_CONTENT': use_content, \n",
    "    \n",
    "    # с какой даты начинается train. Всё более старое не используем для обучения\n",
    "    'CUT_DATE': '2016-01-01' \n",
    "}\n",
    "\n",
    "REGEX_TAGS = re.compile(r\",\\s*\")\n",
    "REGEX_HUBS = re.compile(r\",|/\\s*\")\n",
    "\n",
    "frm = {\n",
    "    '.' : '',\n",
    "    '!' : '',\n",
    "    \"'\" : '',\n",
    "    '\"' : '',\n",
    "    '-' : '',\n",
    "    '#' : '',\n",
    "    '*' : '',\n",
    "    ' ' : ''\n",
    "}\n",
    "TRANS_TABLE = str.maketrans(frm)\n",
    "\n",
    "def tokenize_tags(text):\n",
    "    return [tok.translate(TRANS_TABLE).lower() for tok in REGEX_TAGS.split(text)]\n",
    "\n",
    "def tokenize_hubs(text):\n",
    "    return [tok.translate(TRANS_TABLE).lower() for tok in REGEX_HUBS.split(text)]\n",
    "\n",
    "def preprocess_data(X):\n",
    "    X['published'] = X['published'].apply(lambda x: pd.to_datetime(x, yearfirst=True))\n",
    "    X['content'] = X['content'].astype(str).fillna('-')\n",
    "    X['content_len'] = X['content_len'].fillna(1)\n",
    "    X['title'] = X['title'].astype(str).fillna('-')\n",
    "    X['author_name'] = X['author_name'].fillna('-')\n",
    "    X['author_nickname'] = X['author_nickname'].fillna('-')\n",
    "    #X['author_url'] = X['author_url'].apply(lambda x: '/'.join(x.split('/')[3:]) if not pd.isnull(x) else '')\n",
    "\n",
    "    return X\n",
    "\n",
    "def create_features(X, feature_list):\n",
    "    #не портим переданный список - сделаем копию\n",
    "    fl = feature_list.copy()\n",
    "    while fl:\n",
    "        feature = fl.pop(0)\n",
    "        if feature == 'polling':\n",
    "            X.ix[~pd.isnull(X[feature]), feature] = 1\n",
    "            X[feature] = X[feature].fillna(0).astype(int)\n",
    "\n",
    "        elif feature == 'start_month':\n",
    "            X[feature] = X['published'].apply(lambda ts: 100 * ts.year + ts.month)\n",
    "\n",
    "        elif feature == 'year':\n",
    "            X[feature] = X['published'].apply(lambda ts: ts.year)\n",
    "\n",
    "        elif feature == 'month':\n",
    "            X[feature] = X['published'].apply(lambda ts: ts.month)\n",
    "\n",
    "        elif feature == 'hour':\n",
    "            X[feature] = X['published'].apply(lambda ts: ts.hour)\n",
    "\n",
    "        elif feature == 'hour_min':\n",
    "            X[feature] = X['published'].apply(lambda ts: ts.hour*100 + ts.minute)\n",
    "\n",
    "        elif feature == 'morning':\n",
    "            X[feature] = X['hour'].apply(lambda h: int(h <= 11))\n",
    "\n",
    "        elif feature == 'dayofweek':\n",
    "            X[feature] = X['published'].apply(lambda ts: ts.isoweekday())\n",
    "\n",
    "        elif feature == 'isweekend':\n",
    "            X[feature] = X['dayofweek'].apply(lambda day: 1 if day in [6, 7] else 0)\n",
    "\n",
    "        elif feature == 'isweekend_ev':\n",
    "            X[feature] = X['published'].apply(lambda ts: 1 if ((ts.isoweekday() == 6 or ts.isoweekday() == 7) and ts.hour >= 19) else 0)\n",
    "\n",
    "        elif feature == 'week_hour':\n",
    "            X[feature] = X['published'].apply(lambda ts: ts.weekday()*24 + ts.hour)\n",
    "\n",
    "        elif feature == 'content_len_sqrt':\n",
    "            X[feature] = X['content_len'].apply(lambda x: np.sqrt(x + 1))\n",
    "\n",
    "        elif feature == 'content_len_log':\n",
    "            X[feature] = X['content_len'].apply(lambda x: np.log2(x+1))\n",
    "\n",
    "        elif feature == 'content_len_log**2':\n",
    "            X[feature] = X['content_len_log'].apply(np.square)\n",
    "\n",
    "        elif feature == 'content_len_log**3':\n",
    "            X[feature] = np.power(X['content_len_log'], 3)\n",
    "\n",
    "        elif feature == 'content_len_loglog':\n",
    "            X[feature] = X['content_len_log'].apply(lambda x: np.log(x + 1))\n",
    "\n",
    "        elif feature == 'n_title_length':\n",
    "            X['title'] = X['title'].astype(str).fillna('-')\n",
    "            X['n_title_length'] = X['title'].apply(len)\n",
    "\n",
    "        elif feature == 'n_title_words':\n",
    "            X[feature] = X['title'].apply(lambda s: np.log2(len(s.split()) + 1))\n",
    "\n",
    "        elif feature == 'n_title_words_log':\n",
    "            X[feature] = X['title'].apply(lambda s: np.log2(len(s.split()) + 1))\n",
    "\n",
    "        else:\n",
    "            raise Exception('Feature \"{}\" is not supported'.format(feature))\n",
    "        print('Feature {} created'.format(feature))\n",
    "    return X\n",
    "\n",
    "def print_some_from_vect(v):\n",
    "    \"\"\" Для отладки. Выводим первые 150, последние 150 и 10000:10150 элементы объекта v\n",
    "        v - любой Vectorizer\n",
    "    \"\"\"\n",
    "    vlen = len(v.vocabulary_)\n",
    "    print(vlen)\n",
    "    vec = np.zeros(vlen); vec[:150] = 1; print(v.inverse_transform(vec))\n",
    "    vec = np.zeros(vlen); vec[10000:10150] = 1; print(v.inverse_transform(vec))\n",
    "    vec = np.zeros(vlen); vec[-150:] = 1; print(v.inverse_transform(vec))\n",
    "    return\n",
    "\n",
    "def vectorize_and_scale2(X_train, X_test, categorical_feat_list, features_to_scale=None, features_to_add=None, use_content = False):\n",
    "    print('{} vectorize_and_scale2 started'.format(datetime.datetime.now().time()))\n",
    "\n",
    "    full_df = pd.concat([X_train[categorical_feat_list], X_test[categorical_feat_list]])\n",
    "    oh = OneHotEncoder()\n",
    "    oh.fit(full_df[categorical_feat_list])\n",
    "    X_train_cat = oh.transform(X_train[categorical_feat_list])\n",
    "    X_test_cat = oh.transform(X_test[categorical_feat_list])\n",
    "    print('OneHotEncoder completed')\n",
    "\n",
    "    '''\n",
    "    ## tags\n",
    "    v = CountVectorizer(analyzer='word', tokenizer=tokenize_tags, ngram_range=(1, 1))\n",
    "    X_train_tags = v.fit_transform(X_train['tags'].fillna('-'))\n",
    "    X_test_tags = v.transform(X_test['tags'].fillna('-'))\n",
    "    print('Count vectorizing tags completed')\n",
    "    '''\n",
    "\n",
    "    ## hub_id\n",
    "    v = CountVectorizer(analyzer='word', tokenizer=tokenize_hubs, ngram_range=(1, 1))\n",
    "    X_train_hub_id = v.fit_transform(X_train['hub_id'].fillna('-'))\n",
    "    X_test_hub_id = v.transform(X_test['hub_id'].fillna('-'))\n",
    "    print('Count vectorizing hub_id completed')\n",
    "    #print_some_from_vect(v)\n",
    "\n",
    "    ## flags\n",
    "    v = CountVectorizer(analyzer='word', tokenizer=tokenize_tags, ngram_range=(1, 1))\n",
    "    X_train_flags = v.fit_transform(X_train['flags'].fillna('-'))\n",
    "    X_test_flags = v.transform(X_test['flags'].fillna('-'))\n",
    "    print('Count vectorizing flags completed')\n",
    "\n",
    "    ## title\n",
    "    v = TfidfVectorizer(analyzer='word', min_df=2, max_df=0.3, ngram_range=(1, 2), sublinear_tf=True)\n",
    "    X_train_title = v.fit_transform(X_train['title'])\n",
    "    X_test_title = v.transform(X_test['title'])\n",
    "    print('Title word vectorizing vocabulary size: ', len(v.vocabulary_))\n",
    "    print('Title word vectorizing completed')\n",
    "\n",
    "    v = TfidfVectorizer(analyzer='char', ngram_range=(1, 6), sublinear_tf=True)\n",
    "    X_train_title_ch = v.fit_transform(X_train['title'])\n",
    "    X_test_title_ch = v.transform(X_test['title'])\n",
    "    print('Title char vectorizing vocabulary size: ', len(v.vocabulary_))\n",
    "    print('Title char vectorizing completed')\n",
    "\n",
    "    ## hub_title\n",
    "    ''' \n",
    "    v = TfidfVectorizer(analyzer='word', min_df=2, max_df=0.3, ngram_range=(1, 2), sublinear_tf=True)\n",
    "    X_train_hub_title = v.fit_transform(X_train['hub_title'])\n",
    "    X_test_hub_title = v.transform(X_test['hub_title'])\n",
    "    print('Hub word vectorizing completed')\n",
    "    '''\n",
    "    \n",
    "    v = TfidfVectorizer(analyzer='char', ngram_range=(1, 4), sublinear_tf=True)\n",
    "    X_train_hub_title_ch = v.fit_transform(X_train['hub_title'])\n",
    "    X_test_hub_title_ch = v.transform(X_test['hub_title'])\n",
    "    print('Hub char vectorizing vocabulary size: ', len(v.vocabulary_))\n",
    "    print('Hub char vectorizing completed')\n",
    "\n",
    "    ## content\n",
    "    if use_content:\n",
    "        ngram_low=1; ngram_high=2; min_df=2; max_df=0.3\n",
    "        print('{} Content word vectorizing started.\\n ngram_range=({},{}), min_df={}, max_df={}'.format(\n",
    "            datetime.datetime.now().time(), ngram_low, ngram_high, min_df, max_df))\n",
    "        v = TfidfVectorizer(min_df=min_df, max_df= max_df, ngram_range=(ngram_low, ngram_high), sublinear_tf=True)\n",
    "        X_train_content = v.fit_transform(X_train['content'])\n",
    "        X_test_content = v.transform(X_test['content'])\n",
    "\n",
    "        print('Content word vocabulary size: ', len(v.vocabulary_))\n",
    "        print('{} Content word vectorizing completed'.format(datetime.datetime.now().time()))\n",
    "\n",
    "        ngram_low=1; ngram_high=3\n",
    "        print('{} Content char vectorizing started. ngram_range=({},{})'.format(\n",
    "            datetime.datetime.now().time(), ngram_low, ngram_high))\n",
    "        v = TfidfVectorizer(analyzer='char', ngram_range=(ngram_low, ngram_high), sublinear_tf=True)\n",
    "        X_train_content_ch = v.fit_transform(X_train['content'])\n",
    "        X_test_content_ch = v.transform(X_test['content'])\n",
    "\n",
    "        print('Content char vocabulary size: ', len(v.vocabulary_))\n",
    "        print('{} Content char vectorizing completed'.format(datetime.datetime.now().time()))\n",
    "\n",
    "    # Стыкуем Vectorizers и OneHot\n",
    "    X_train_sparse = scipy.sparse.hstack([\n",
    "        X_train_cat, X_train_hub_id, X_train_flags,\n",
    "        X_train_title, X_train_title_ch, X_train_hub_title_ch])  #X_train_hub_title, X_train_feats, X_train_tags, X_train_flags,\n",
    "    X_test_sparse = scipy.sparse.hstack([\n",
    "        X_test_cat, X_test_hub_id, X_test_flags,\n",
    "        X_test_title, X_test_title_ch, X_test_hub_title_ch])  # X_test_hub_title, X_test_feats,  X_test_tags, X_test_flags,\n",
    "\n",
    "    if use_content:\n",
    "        X_train_sparse = scipy.sparse.hstack([X_train_sparse, X_train_content, X_train_content_ch])\n",
    "        X_test_sparse = scipy.sparse.hstack([X_test_sparse, X_test_content, X_test_content_ch])\n",
    "\n",
    "    # Стыкуем отнормированные фичи\n",
    "    if features_to_scale:\n",
    "        X_train_sparse = scipy.sparse.hstack([\n",
    "            X_train_sparse, StandardScaler().fit_transform(X_train[features_to_scale])])\n",
    "        X_test_sparse = scipy.sparse.hstack([\n",
    "            X_test_sparse, StandardScaler().fit_transform(X_test[features_to_scale])])\n",
    "\n",
    "    # Стыкуем остальные фичи\n",
    "    if features_to_add:\n",
    "        X_train_sparse = scipy.sparse.hstack([X_train_sparse, X_train[features_to_add]])\n",
    "        X_test_sparse = scipy.sparse.hstack([X_test_sparse, X_test[features_to_add]])\n",
    "\n",
    "    return X_train_sparse, X_test_sparse\n",
    "\n",
    "def encode_categorial_features(train_df, test_df, cat_feature_list):\n",
    "    full_df = pd.concat([train_df.drop('favs_lognorm', axis=1), test_df])\n",
    "    for f in cat_feature_list:\n",
    "        lb = LabelEncoder()\n",
    "        lb.fit(full_df[f].values.flatten())\n",
    "        train_df[f] = lb.transform(train_df[f])\n",
    "        test_df[f] = lb.transform(test_df[f])\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    print('{} Начали'.format(datetime.datetime.now().time()))\n",
    "    if cfg['LOAD_CONTENT']:\n",
    "        train_df = pd.read_json(train_file_content)\n",
    "        test_df  = pd.read_json(test_file_content)\n",
    "    else:\n",
    "        train_df = pd.read_json(train_file_light)\n",
    "        test_df  = pd.read_json(test_file_light)\n",
    "\n",
    "    train_df = preprocess_data(train_df)\n",
    "    test_df = preprocess_data(test_df)\n",
    "\n",
    "    # Отрежем старые данные\n",
    "    if cfg['CUT_DATE']:\n",
    "        train_df = train_df[train_df['published'] >= pd.to_datetime(cfg['CUT_DATE'])]\n",
    "    \n",
    "    # генерим дополнительные фичи\n",
    "    new_features = ['dayofweek', 'content_len_log', 'content_len_log**2', 'isweekend_ev']\n",
    "    train_df = create_features(train_df, new_features)\n",
    "    test_df = create_features(test_df, new_features)\n",
    "\n",
    "    ### Все фичи, которые будем использовать помимо фич для Vectorizers\n",
    "    # c масштабированием\n",
    "    features_to_scale = ['isweekend_ev', 'content_len', 'content_len_log', 'content_len_log**2']\n",
    "    # без масштабирования\n",
    "    features_to_add = []\n",
    "    # категориальные\n",
    "    categorical_feat_list = ['author_nickname', 'domain', 'dayofweek']\n",
    "\n",
    "    train_df, test_df = encode_categorial_features(train_df, test_df, categorical_feat_list)\n",
    "\n",
    "    print('{} PREDICT начат'.format(datetime.datetime.now().time()))\n",
    "    # Целевые веременные\n",
    "    y_train = train_df['favs_lognorm']\n",
    "    train_df.drop('favs_lognorm', axis=1)\n",
    "\n",
    "    # Соберем всё вместе\n",
    "    X_train_sparse, X_test_sparse = vectorize_and_scale2(\n",
    "        train_df,\n",
    "        test_df,\n",
    "        categorical_feat_list,\n",
    "        features_to_scale,\n",
    "        features_to_add,\n",
    "        cfg['LOAD_CONTENT']\n",
    "    )\n",
    "    print('{} PREDICT. Данные загрузили. Размер тренировочной выборки {}'.format(datetime.datetime.now().time(), X_train_sparse.shape))\n",
    "\n",
    "    alpha = 1\n",
    "    model = Ridge(alpha=alpha, random_state=seed)\n",
    "    model.fit(X_train_sparse, y_train)\n",
    "    test_preds = model.predict(X_test_sparse)\n",
    "    print('{} PREDICT завершен'.format(datetime.datetime.now().time()))\n",
    "\n",
    "    # Готовим submission \n",
    "    submission = pd.DataFrame()\n",
    "    submission['_id'] = test_df._id.values\n",
    "    submission['favs_lognorm'] = test_preds\n",
    "    submission.to_csv(submission_file, index=False)\n",
    "    print('WRITE_SUBMISSION. Файл записан')\n",
    "    \n",
    "    # работает только на Mac :)\n",
    "    os.system('say \"your program has finished\"')\n",
    "\n",
    "except:\n",
    "    os.system('say \"something has gone wrong\"')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. Заключение "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Что больше всего помогало улучшению результата:\n",
    "- отрезать все старые данные. \n",
    " Я сильно не оптимизировал, попробовал разные варианты с шагом полгода, лучше всего оказалось начинать с 01.01.2016. Подозреваю, что старые статьи отличаются по содержанию, плюс по старым данным довольно много пропусков и отличается структура в нескольких фичах: хабы, флаги. Возможно и сценарии использования  избранного в первые годы существования Хабра отличались. Это дало в районе +0.15 к результату\n",
    "- подобрать параметры для tf-idf. Это касается и содержания статей и их заголовков и хабов. Подбор параметров довольно сильно улучшил результат, в частности изменение значения min_df c 3 (значение из бэйзлайна в 4 домашке) на 2. \n",
    "- фичи производные от длины статьи. Я попробовал разные варианты, в итоге использовал саму длину, log длины и log^2 от длины. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
